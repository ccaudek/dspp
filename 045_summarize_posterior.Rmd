# Sintesi a posteriori {#chapter-sintesi-distr-post}

```{r, include = FALSE}
source("_common.R")
theme_set(bayesplot::theme_default())
bayesplot::color_scheme_set("brightblue") 
library("rstan")
library("cmdstanr")
set_cmdstan_path("/Users/corrado/.cmdstanr/cmdstan-2.27.0")
library("posterior")
rstan_options(auto_write = TRUE) # avoid recompilation of models
options(mc.cores = parallel::detectCores()) # parallelize across all CPUs
Sys.setenv(LOCAL_CPPFLAGS = '-march=native') # improve execution time
SEED <- 374237 # set random seed for reproducibility
theme_set(bayesplot::theme_default(base_family = "sans"))
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE,
  error = FALSE,
  fig.align = "center"
)
```

La distribuzione a posteriori è un modo per descrivere il nostro grado
di incertezza rispetto al parametro incognito (o rispetto ai parametri
incogniti) oggetto dell'inferenza. La distribuzione a posteriori
contiene tutte le informazioni disponibili sui possibili valori del
parametro. Se il parametro esaminato è monodimensionale (o
bidimensionale) è possibile fornire un grafico di tutta la distribuzione a posteriori $p(\theta \mid y)$. Tuttavia, spesso vogliamo anche giungere ad una sintesi numerica della distribuzione a posteriori, soprattutto se il vettore dei parametri ha più di due dimensioni. A a questo proposito è possibile utilizzare le consuete statistiche descrittive, come media, mediana, moda, varianza, deviazione standard e i quantili. In alcuni casi, queste statistiche descrittive sono più facili da presentare e interpretare rispetto alla rappresentazione grafica completa della distribuzione a posteriori.

La stima puntuale della distribuzione a posteriori fornisce informazioni su quello che può essere considerato come il "valore più plausibile" del parametro. L'intervallo di credibilità fornisce invece un'indicazione dell'ampiezza dell'intervallo che contiene una determinata quota della massa della distribuzione a posteriori del parametro.


## Stima puntuale 

Per sintetizzare la distribuzione a posteriori in modo da giungere ad
una stima puntuale di $\theta$ si è soliti scegliere tra moda, mediana o media a seconda del tipo di distribuzione con cui si ha a che fare e
della sua forma. Ogni stima puntuale ha una sua interpretazione. La media è il valore atteso a posteriori del parametro. La moda può essere interpretata come il singolo valore più credibile ("più plausibile") del parametro, alla luce dei dati, ovvero il valore per il parametro $\theta$ che massimizza la distribuzione a posteriori. Per questa ragione la moda viene detta *massimo a posteriori*, MAP. Il limite della moda quale statistica riassuntiva della distribuzione a posteriori è che, talvolta, tale distribuzione è multimodale e il MAP non è necessariamente il valore "più credibile". Infine, la mediana è il valore del parametro tale per cui, su entrambi i lati di essa, giace il 50% della massa di probabilità a posteriori.

La misura di variabilità per il parametro è la *varianza a posteriori*
la quale, nel caso di una distribuzione a posteriori ottenuta per via
numerica, si calcola con la formula della varianza che conosciamo
rispetto alla tendenza centrale data dalla media a posteriori. La radice quadrata della varianza a posteriori è la *deviazione standard a posteriori* che fornisce l'incertezza a posteriori circa il parametro di interesse il quale, in un'ottica bayesiana, è una variabile casuale.

Tutte le procedure bayesiane si basano su sull'uso di metodi MCMC per
ottenere le stime a posteriori. Usando un numero finito di campioni, le
simulazioni introducono un ulteriore livello di incertezza
sull'accuratezza della stima. L'*errore standard della stima* (in
inglese *Monte Carlo standard error*, MCSE) misura l'accuratezza della
simulazione. La deviazione standard a posteriori e l'errore standard
della stima sono due concetti molto diversi. La deviazione standard a
posteriori descrive l'incertezza circa il parametro ed è una funzione
della dimensione del campione; il MCSE descrive invece l'incertezza
nella stima del parametro come risultato della simulazione MCMC ed è una funzione del numero di iterazioni nella simulazione.


## Intervallo di credibilità 

Molto spesso la stima puntuale è accompagnata da una stima intervallare. Nella statistica bayesiana, se il parametro $\theta \in \Theta$ è monodimensionale, si dice _intervallo di credibilità_ un intervallo di valori $I_{\alpha}$ che contiene la proporzione $1 - \alpha$ della massa di probabilità della funzione a posteriori:

\begin{equation}
p(\Theta \in I_{\alpha} \mid y) = 1 - \alpha.
(\#eq:credibint)
\end{equation}

L'intervallo di credibilità ha lo scopo di esprimere il nostro grado di incertezza riguardo la stima del parametro. Se il parametro $\theta$ è multidimensionale, si parla invece di "regione di credibilità".

La condizione \@ref(eq:credibint) non determina un unico intervallo di
credibilità al $(1 - \alpha) \cdot 100\%$. In realtà esiste un numero
infinito di tali intervalli. Ciò significa che dobbiamo definire alcune
condizioni aggiuntive per la scelta dell'intervallo di credibilità.
Esaminiamo due delle condizioni aggiuntive più comuni.


### Intervallo di credibilità a code uguali 

Un intervallo di credibilità *a code uguali* a livello $\alpha$ è un
intervallo $$I_{\alpha} = [q_{\alpha/2}, 1 - q_{\alpha/2}],$$ dove $q_z$ è un quantile $z$ della distribuzione a posteriori. Per esempio,
l'intervallo di credibilità a code uguali al 95% è un intervallo
$$I_{0.05} = [q_{0.025}, q_{0.975}]$$ che lascia il 2.5% della massa di
densità a posteriori in ciascuna coda. 


### Intervallo di credibilità a densità a posteriori più alta 

Nell'intervallo di credibilità a code uguali alcuni valori del parametro che sono inclusi nell'intervallo possono avere una più bassa probabilità a posteriori rispetto a quelli fuori dell'intervallo. L'intrevallo di credibilità *a densità a posteriori più alta* (in inglese *High Posterior Density Interval*, HPD) è invece costruito in modo tale da assicurare di avere all'interno dell'intervallo tutti i valori di $\theta$ che sono a posteriori più plausibili. Graficamente questo intervallo può essere ricavato tracciando una linea orizzontale sulla rappresentazione della distribuzione a posteriori e regolando l'altezza della linea in modo tale che l'area sotto la curva sia pari a $1 - \alpha$. Questo tipo di intervallo è il meno ampio che si possa
determinare e inoltre se la distribuzione a posteriori è simmetrica
unimodale l'intervallo di credibilità a densità a posteriori più alta
corrisponde all'intervallo di credibilità a code uguali.


### Interpretazione 

L'interpretazione dell'intervallo di credibilità è molto intuitiva:
l'intervallo di credibilità è un intervallo di valori all'interno del
quale cade il valore del parametro incognito con un particolare livello
di probabilità soggettiva. Possiamo dire che, dopo aver visto i dati
crediamo, con un determinato livello di probabilità soggettiva, che il
valore del parametro (ad esempio, la dimensione dell'effetto di un
trattamento) abbia un valore compreso all'interno dell'intervallo che è
stato calcolato, laddove per probabilità soggettiva intendiamo "il grado di fiducia che lo sperimentatore ripone nel verificarsi di un evento". Solitamente gli intervalli di credibilità si calcolano con un software.


## Un esempio concreto 

Per fare un esempio pratico, consideriamo i 30 valori del BDI-II dei soggetti clinici di @zetschefuture2019:

```{r}
suppressPackageStartupMessages(library("bayesrules")) 

df <- data.frame(
  y = c(26, 35, 30, 25, 44, 30, 33, 43, 22, 43, 
        24, 19, 39, 31, 25, 28, 35, 30, 26, 31, 
        41, 36, 26, 35, 33, 28, 27, 34, 27, 22)
)
```

Un valore BDI-II $\geq 30$ indica la presenza di un livello "grave" di depressione. Nel campione clinico di @zetschefuture2019, 

```{r}
sum(df$y > 29)
```

\noindent
17 pazienti su 30 manifestano un livello grave di depressione.

Supponiamo di volere stimare la distribuzione a posteriori della probabilità $\theta$ di depressione "grave" nei pazienti clinici, così come viene misurata dal test BDI-II, imponendo su $\theta$ una distribuzione a priori Beta(8, 2). 

Sappiamo che il modello Beta-Binomiale può essere espresso nella forma seguente:

\begin{align}
Y | \pi & \sim \text{Bin}(30, \pi) \notag\\
\pi & \sim \text{Beta}(8, 2) \notag
\end{align} 

\noindent
con una corrispondente distribuzione a posteriori Beta(25, 15):

\begin{equation}
f(\pi | y = 17) = \frac{\Gamma(25 + 15)}{\Gamma(25)\Gamma(15)}\pi^{25-1} (1-\pi)^{15-1} \;\; \text{ for } \pi \in [0,1] \; .
(\#eq:post-beta-25-15)
\end{equation}

```{r}
plot_beta_binomial(alpha = 8, beta = 2, y = 17, n = 30)
```


### Stime puntuali della distribuzione a posteriori

Una volta trovata l'intera distribuzione a posteriori, quale valore di
sintesi è necessario riportare? Questa sembra una domanda innocente, ma
in realtà è una domanda a cui è difficile rispondere. La stima bayesiana dei parametri è fornita dall'intera distribuzione a posteriori, che non è un singolo numero, ma una funzione che mappa ciascun valore del parametro di interesse ad un valore di plausibilità. Quindi non è necessario scegliere una stima puntuale. In linea di
principio, una stima puntuale non è quasi mai necessaria ed è spesso dannosa in quanto comporta una perdita di informazioni.

Tuttavia talvolta una tale sintesi è richiesta. Diverse risposte sono allora possibili. La media della distribuzione a posteriori per $\theta$ è

$$
\E(\pi \mid y = 17) = \frac{\alpha}{\alpha + \beta} = \frac{25}{25+15} = 0.625.
$$

Una stima del massimo della probabilità a posteriori, o brevemente massimo a posteriori, MAP (da *maximum a posteriori probability*), è la moda della distribuzione a posteriori. Nel caso presente, una stima del MAP può essere ottenuta nel modo seguente:

$$
\Mo(\pi \mid y = 17) = \frac{\alpha-1}{\alpha + \beta-2} = \frac{25-1}{25+15-2} = 0.6316.
$$

Gli stessi risultati si ottiengono usando la chiamata a `bayesrules::summarize_beta_binomial()`:

```{r}
summarize_beta_binomial(alpha = 8, beta = 2, y = 17, n = 30)
```

La mediana si ottiene con 

```{r}
qbeta(.5, shape1 = 25, shape2 = 15)
```


### Intervallo di credibilità

È più comune sintetizzare la distribuzione a posteriori mediante un
intervallo di valori, detto "intervallo di credibilità". Per esempio, l'intervallo di credibilità a code uguali al 95% 

```{r}
plot_beta_ci(alpha = 25, beta = 15, ci_level = 0.95)
```

\noindent
è dato dalla chiamata

```{r}
qbeta(c(0.025, 0.975), 25, 15)
```
Il calcolo precedente evidenzia l'interpretazione intuitiva dell'intervallo di credibilità. Esso infatti può essere interpretato come la probabilità che $\theta$ assuma valori compresi tra 0.472 e 0.766:

$$
P(\theta \in (0.472, 0.766) | Y = 17) = \int_{0.472}^{0.766} f(\theta \mid y=17) d\theta = 0.95,
$$

\noindent
ovvero

```{r}
postFun <- function(theta) {
  gamma(25 + 15) / (gamma(25) * gamma(15)) * theta^24 * (1 - theta)^14
}
integrate(
  postFun, 
  lower = 0.4717951, 
  upper = 0.7663607
)$value
```

Possiamo ovviamente costruire diversi intervalli di credibilità a code equivalenti. Ad esempio, l'intervallo di credibilità compreso tra il 25-esimo e il 75-esimo percentile è

```{r}
qbeta(c(0.25, 0.75), 25, 15)
```

\noindent
ovvero, abbiamo una certezza a posteriori del 50% che la probabilità di depressione grave tra i pazienti clinici sia un valore compreso tra 0.57 e 0.68.

Non esiste un livello credibile "corretto". I ricercatori, utilizzano i livelli del 50%, 80% o 95%, a seconda del contesto dell'analisi. Ciascuno di questi intervalli fornisce un'istantanea diversa della nostra comprensione della distribuzione a posteriori del parametro di interesse.

Non è inoltre necessario riportare l'intervallo di credibilità a code uguali.  Se la distribuzione a posteriori è fortemente asimmetrica è più sensato riportare l'intrevallo di credibilità a densità a posteriori più alta. È più semplice calcolare l'intervallo HPD quando la distribuzione a posteriori viene approssimata con il metodo MCMC.


### Probabilità della distribuzione a posteriori

Il test di ipotesi è anche un compito comune dell'analisi della distribuzione a posteriori. Supponiamo che si voglia conoscere la probabilità a posteriori che $\theta$ sia superiore a 0.5. Per sapere quanto plausibile sia l'evento $\theta > 0.5$ possiamo calcolare il seguente integrale:

$$
P(\theta > 0.5 \; \mid \; y = 17) = \int_{0.5}^{1}f(\theta \mid y=17)d\theta \;,
$$

\noindent
dove $f(\cdot)$ è la distribuzione $\Beta(25, 15)$:

```{r}
pbeta(0.5, shape1 = 25, shape2 = 15, lower.tail = FALSE)
```

\noindent
il che è equivalente a:

```{r}
postFun <- function(theta) {
  gamma(25 + 15) / (gamma(25) * gamma(15)) * theta^24 * (1 - theta)^14
}
integrate(
  postFun, 
  lower = 0.5, 
  upper = 1
)$value
```

È anche possibile formulare il test di ipotesi contrastando due ipotesi contrapposte. Per esempio, $H_1: \theta \geq 0.5$ e $H_2: \theta < 0.5$. A questo punto diventa possibile calcolare l'_odds a posteriori_ che $\theta > 0.5$:

\begin{equation}
\text{poterior odds} = \frac{H_1 \mid y = 17}{H_2 \mid y = 17}
\end{equation}

\noindent
ovvero

```{r}
posterior_odds <- 
  pbeta(0.5, shape1 = 25, shape2 = 15, lower.tail = FALSE) /
  pbeta(0.5, shape1 = 25, shape2 = 15, lower.tail = TRUE)
posterior_odds
```

\noindent
L'odds a posteriori rappresenta l'aggiornamento delle nostre credenze dopo avere osservato $y = 17$ su $n=30$. L'odds a priori che $\theta > 0.5$ era:

```{r}
prior_odds <- 
  pbeta(0.5, shape1 = 8, shape2 = 2, lower.tail = FALSE) /
  pbeta(0.5, shape1 = 8, shape2 = 2, lower.tail = TRUE)
prior_odds
```

Il fattore di Bayes (_Bayes Factor_; BF) confronta gli odds a posteriori con gli odds a priori, quindi fornisce informazioni su quanto è mutata la nostra comprensione relativa a $\theta$ avendo osservato i nostri dati del campione:

$$
\text{BF} = \frac{\text{odds a posteriori}}{\text{odds a priori}}.
$$

\noindent
Nel caso presente abbiamo

```{r}
BF <- posterior_odds / prior_odds
BF
```

Quindi, dopo avere osservato i dati, gli odds a posteriori della nostra ipotesi a proposito di $\theta$  sono pari a solo il 34% degli odds a priori. 

Per fare un altro esempio, consideriamo invece il caso in cui le credenze a priori rivelano una credenza diametralmente opposta a quella considerata in precedenza per $\theta$, ovvero Beta(2, 8). In questo secondo caso, la distribuzione a posteriori diventa

```{r}
summarize_beta_binomial(alpha = 2, beta = 8, y = 17, n = 30)
```

\noindent
e il BF è

```{r}
posterior_odds <- 
  pbeta(0.5, shape1 = 19, shape2 = 21, lower.tail = FALSE) /
  pbeta(0.5, shape1 = 19, shape2 = 21, lower.tail = TRUE)

prior_odds <- 
  pbeta(0.5, shape1 = 2, shape2 = 8, lower.tail = FALSE) /
  pbeta(0.5, shape1 = 2, shape2 = 8, lower.tail = TRUE)

BF <- posterior_odds / prior_odds
BF
```

\noindent
In alre parole, in questo secondo esempio gli odds a posteriori della nostra ipotesi a proposito di $\theta$ sono aumentati di 30 volte rispetto agli odds a priori.

In generale, in un test di ipotesi che contrappone un'ipotesi sostantiva $H_a$ ad un'ipotesi nulla $H_0$ il BF è un rapporto di odds per l'ipotesi sostantiva:

$$
\text{Bayes Factor}
= \frac{\text{posterior odds}}{\text{prior odds}}
= \frac{P(H_a \mid Y) / P(H_0 \mid Y)}{P(H_a) / P(H_0)}
 \; .
$$

\noindent
Essendo un rapporto, il BF deve esere valutato rispetto al valore di 1. Ci sono tre possibilità: 

- BF = 1: La plausibilità di $H_a$ non è cambiata dopo avere osservato i dati. 
- BF > 1: La plausibilità di $H_a$ è aumentata dopo avere osservato i dati. Quindi maggiore è BF, più convincente risulta l'evidenza per $H_a$.
- BF < 1: La plausibilità di $H_a$ è diminuita dopo avere osservato i dati.

\noindent
Non ci sono delle soglie universalmente riconosciute per interpretare il BF. Per esempio, @lee2014bayesian propongono il seguente schema:

| BF | Interpretation |
|--: |:--|
> 100	| Extreme evidence for $H_a$
30 - 100 |	Very strong evidence for $H_a$
10 - 30 |	Strong evidence for $H_a$
3 - 10 |	Moderate evidence for $H_a$
1 - 3	| Anecdotal evidence for $H_a$
1	| No evidence
1/3 - 1	| Anecdotal evidence for $H_0$
1/10 - 1/3 |	Moderate evidence for $H_0$
1/30 - 1/10	| Strong evidence for $H_0$
1/100 - 1/30 |	Very strong evidence for $H_0$
< 1/100	| Extreme evidence for $H_0$

\noindent
Tuttavia, l'opinione maggiormente diffusa nella comunità scientifica è quella che incoraggia a non trarre conclusioni rigide  dai dati utilizzando dei criteri universali fissati una volta per tutte. Pertanto, non esiste una soglia univoca per il BF che consenta di suddividere le ipotesi dei ricercatori nelle due categorie di "vero" o "falso". Invece, è più utile adottare una pratica maggiormente flessibile tenendo in considerazione il contesto e le potenziali implicazioni di ogni singolo test di ipotesi. Inoltre, la distribuzione a posteriori è molto più informativa di una decisione binaria: ci fornisce una misura olistica del nostro livello di incertezza riguardo all'affermazione (il parametro) che viene valutata. 


## Distribuzione predittiva a posteriori

Oltre alla stima a posteriori degli indici sintetici del parametro e alla verifica delle ipotesi, un terzo compito comune in un'analisi bayesiana è la predizione di nuovi dati futuri.

::: {.definition}
Si chiama _distribuzione predittiva a posteriori_ (_posterior predictive distribution_, PPD), la distribuzione di future osservazioni di dati, ovvero la distribuzione sui possibili dati previsti futuri $\tilde{y}$ data la distribuzione a posteriori per $\theta$ che è stata ottenuta. La distribuzione predittiva a posteriori $p(\tilde{y} \mid y)$ è
\begin{equation}
p(\tilde{y} \mid y) = \int p(\tilde{y} \mid \theta) p(\theta \mid y) d\theta.
(\#eq:dist-pred-post)
\end{equation}
:::

La \@ref(eq:dist-pred-post) rappresenta la nostra incertezza sui dati previsti futuri, tenendo conto della scelta del modello e della stima dei parametri mediante i dati osservati.


## Un esempio concreto

Consideriamo nuovamente il campione di pazienti clinici depressi  studiati da @zetschefuture2019. Supponiamo che i ricercatori vogliano osservare in futuro altri 20 pazienti clinici. Sia $\tilde{y}$ il numero di pazienti che manifestano una depressione grave, laddove $\tilde{y} \in \{0, 1, \dots, 20\}$. Se vogliamo fare una predizione su $\tilde{y}$ (il numero di depressi gravi su 20), è chiaro però che i valori nell'intervallo [0, 20] non hanno tutti la stessa plausibilità. La variabile $\tilde{y}$ è una v.c. binomiale; sappiamo che $\tilde{Y} \mid \theta \sim \Bin(20, \theta)$ con densità

\begin{equation}
p(y'\mid \theta) = p(Y' = y' \mid \theta) = \binom{20}{y'} \theta^{y'}(1-\theta)^{20 - y'} \; .
(\#eq:post-yprime)
\end{equation}

\noindent
Quindi, la v.c. $Y'$ dipende da $\theta$. Ma $\theta$ è anch'essa una variabile casuale. La **potenziale** distribuzione a posteriori di $\theta$, alla luce dei dati originari ($y=14$), è una $\Beta(25, 15)$ -- questa distribuzione descrive la plausibilità relativa a posteriori dei valori $\theta$. Per trovare la verosimiglianza relativa dei valori $\tilde{y}$ per le  previste future 20 osservazioni è necessario applicare la \@ref(eq:dist-pred-post):

\begin{align}
p(\tilde{y} \mid y = 17) = \int_0^1 p(\tilde{y} \mid \theta) p(\theta \mid y = 17) d\theta.
(\#eq:post-yprime-y17)
\end{align}

\noindent
In parole, ciò significa che dobbiamo "sommare" la funzione $p(\tilde{y} \mid \theta)$ assegnando a $\theta$ tutti i valori possibili in [0, 1], ponderando  ciascuno di questi "addendi" con un peso corrispondente alla verosimiglianza di $\theta$ nella distribuzione $p(\theta \mid y = 17)$. 

\noindent
Per il modello binomiale con una distribuzione a priori Beta è semplice ricavare analiticamente la distribuzione predittiva a posteriori:

\begin{align}
p(\tilde{y} \mid y) &= \int_0^1 p(\tilde{y} \mid \theta)
p(\theta \mid y)\, d\theta \notag\\
 &= \int_0^1 \begin{pmatrix}\tilde{n}\\\tilde{y}\end{pmatrix}
 \theta^{\tilde{y}}
(1-\theta)^{\tilde{n}-\tilde{y}} \Beta(a+y,b+n-y) \, d\theta \notag\\
&= \begin{pmatrix}{\tilde{n}}\\\tilde{y}\end{pmatrix} \int_0^1 \theta^{\tilde{y}}
(1-\theta)^{\tilde{n}-\tilde{y}} \frac{1}{B(a+y, b+n-y)}\theta^{a+y-1}(1-\theta)^{b+n-y-1}\notag\\
&= \begin{pmatrix}{\tilde{n}}\\\tilde{y}\end{pmatrix} \frac{1}{B(a+y, b+n-y)}\int_0^1 \theta^{\tilde{y}+a+y-1}(1-\theta)^{\tilde{n}-\tilde{y}+b+n-y-1}\notag\\
&= \begin{pmatrix}{\tilde{n}}\\\tilde{y}\end{pmatrix} \frac{B(\tilde{y}+a+y,b+n-y+\tilde{n}-\tilde{y})}{B(a+y, b+n-y)} 
(\#eq:post-yprime-an-sol-betabin)
\end{align}

Per il caso presente, otteniamo:

```{r}
# Beta Binomial Predictive distribution function
# https://rpubs.com/FJRubio/BetaBinomialPred
BetaBinom <- Vectorize(
  function(rp){
  log_val <- lchoose(np, rp) + 
    lbeta(rp+a+y, b+n-y+np-rp) - 
    lbeta(a+y, b+n-y)
  return(exp(log_val))
  }
)

n <- 30; y <- 17; a <- 25; b <- 15; np <- 20
data.frame(
  heads = 0:20, 
  pmf = BetaBinom(0:20)
) %>%
ggplot(aes(x = factor(heads), y = pmf)) +
  geom_col() +
  geom_text(
    aes(label = round(pmf, 2), y = pmf + 0.01),
    position = position_dodge(0.9),
    size = 3,
    vjust = 0
  ) +
  labs(title = "Distribuzione predittiva a posteriori",
       x = "y'",
       y = "P(Y = y' | Data)") 
```

Nel caso dell'esempio che stiamo discutendo, la distribuzione predittiva a posteriori $p(\tilde{y} \mid y)$ è simile alla distribuzione binomiale di parametro $\theta = 0.63$ (anche se non è identica ad essa). In particolare, la $p(\tilde{y} \mid y)$ ha una varianza maggiore di $\Bin(\tilde{n} = 20, \theta = 0.63)$:

```{r}
data.frame(
  heads = 0:20, 
  pmf = dbinom(x = 0:20, size = 20, prob = 0.63)
) %>%
ggplot(aes(x = factor(heads), y = pmf)) +
  geom_col() +
  geom_text(
    aes(label = round(pmf,2), y = pmf + 0.01),
    position = position_dodge(0.9),
    size = 3,
    vjust = 0
  ) +
  labs(title = "p(y | theta = 0.63)",
       x = "y",
       y = "probability") 
```

Questa maggiore varianza riflette le due fonti di incertezza che sono presenti nella \@ref(eq:dist-pred-post): l'incertezza sul valore del parametro (descritta dalla distribuzione a posteriori) e l'incertezza dovuta alla variabilità campionaria (descritta dalla funzione di verosimiglianza). 

\noindent
Possiamo dunque concludere che, nel caso venissero osservati 20 nuovi pazienti clinici, il numero più plausibile di pazienti che manifestano una depressione severa (secondo il BDI-II) è 12, anche se è ragionevole aspettarsi un numero compreso, diciamo, tra 8 e 16.

\noindent
Una volta trovata la distribuzione predittiva a posteriori $p(\tilde{y} \mid y)$ diventa possibile rispondere a domande come: qual è la probabilità che almeno 10 dei 20 pazienti futuri mostrino una depressione grave, $P(\tilde{Y} \geq 10 \mid Y = 17)$? Oppure: quanti dei 20 pazienti futuri ci possiamo aspettare che mostrino una depressione grave, $\E(\tilde{Y} \mid Y = 17)$?

\noindent
Rispondere a domande di questo tipo è possibile, avendo a disposizione la \@ref(eq:post-yprime-an-sol-betabin), ma richiede un po' di lavoro -- non ci sono funzioni R che svolgano questi calcoli per noi. Tuttavia, non è importante imparare a risolvere problemi di questo tipo perché, in generale, anche per problemi solo leggermente più complessi di quello discusso qui, non disponiamo di una espressione analitica della distribuzione predittiva a posteriori $p(\tilde{y} \mid y)$ [come nel caso della \@ref(eq:post-yprime-an-sol-betabin)]. Invece, in generale è necessario fare affidamento sulla simulazione MCMC per ottenere in maniera numerica una approssimazione della distribuzione predittiva a posteriori. In tali circostanze, è molto più facile trovare risposta a domande come le precedenti.


## Metodi MCMC per la distribuzione predittiva a posteriori 

I Paragrafi precedenti hanno illustrato la teoria statistica alla base dell'analisi bayesiana della distribuzione a posteriori. Quando lavoriamo con modelli semplici come il modello Beta-Binomiale, possiamo implementare direttamente le teoria che sta alla base del modello, ovvero possiamo calcolare in maniera esatta gli intervalli di credibilità, le probabilità a posteriori e le proprietà della distribuzione predittiva a posteriori. 

Tuttavia, espressioni analitiche che consentono una soluzione esatta ai problemi elencati sopra sono disponibili solo per modelli molto semplici, come ad esempio quello Beta-Binomiale. In generale, dobbiamo ricorrere ai metodi di campionamento MCMC per ottenere una approssimazione numerica della distribuzione a posteriori. In tali circostanze, sono stati messi a punto dei metodi che consentono di trovare una approsimazione anche della distribuzione predittiva a posteriori. Dunque, per rispondere alle domande di interesse non sarà necessario applicare delle formule ma potremo invece usare delle funzioni R. 

Consideriamo nuovamente il codice Stan per una proporzione che abbiamo discusso nel Capitolo \@ref(mod-binom):

```{r}
modelString = "
data {
  int<lower=0> N;
  int<lower=0, upper=1> y[N];
}
parameters {
  real<lower=0, upper=1> theta;
}
model {
  theta ~ beta(2, 2);
  y ~ bernoulli(theta);
}
generated quantities {
  int y_rep[N];
  real log_lik[N];
  for (n in 1:N) {
    y_rep[n] = bernoulli_rng(theta);
    log_lik[n] = bernoulli_lpmf(y[n] | theta);
  }
}
"
writeLines(modelString, con = "code/oneprop1.stan")
```

Utilizzando la notazione di @gelman2014understanding, chiamiamo $y^{rep}$ i dati previsti futuri che potrebbero venire osservati se l'esperimento casuale che ha prodotto $y$ venisse ripetuto, ovvero una realizzazione futura del modello statistico con gli stessi valori dei parametri $\theta$ che hanno prodotto $y$.  @gelman2014understanding distinguono $y^{rep}$ (repliche sotto lo stesso modello statistico) da $\tilde{y}$, che corrisponde invece ad un effettivo campione empirico di dati osservato in qualche futura occasione. 

\noindent
Quando l'analisi bayesiana viene svolta mediante metodi MCMC, una stima della distribuzione predittiva a posteriori si ottiene nel modo seguente:

- campionare $\theta_i \sim p(\theta \mid y)$, ovvero campionare un valore del parametro dalla distribuzione a posteriori;
- campionare $y^{rep} \sim p(y^{rep} \mid \theta_i)$, ovvero campionare il valore di un'osservazione dalla funzione di verosimiglianza condizionata al valore del parametro definito nel passo precedente.

Questo processo in due fasi riflette le due fonti di incertezza che sono presenti: l'_incertezza sul valore del parametro_ (riflessa dalla distribuzione a posteriori) e l'_incertezza dovuta alla variabilità campionaria_ (riflessa dalla funzione di verosimiglianza). Se i due passaggi descritti sopra vengono ripetuti un numero sufficiente di volte, l'istogramma risultante approssimerà la distribuzione predittiva a posteriori che, in teoria (ma non in pratica) potrebbe essere ottenuta per via analitica. 

Le istruzioni necessarie per simulare $y^{rep}$ (ovvero, `y_rep[n] = bernoulli_rng(theta);` sono state aggiunte nel blocco `generated quantities` del codice Stan.

I dati dell'esempio che stiamo discutendo sono:

```{r}
data_list <- list(
  N = 30,
  y = c(rep(1, 17), rep(0, 13))
)
```

\noindent
Compiliamo il codice Stan:

```{r, message=FALSE, comment=FALSE, error=FALSE}
file <- file.path("code", "oneprop1.stan")
mod <- cmdstan_model(file)
```

\noindent
ed eseguiamo il campionamento MCMC: 

```{r, message=FALSE, comment=FALSE, error=FALSE}
fit <- mod$sample(
  data = data_list,
  iter_sampling = 4000L,
  iter_warmup = 2000L,
  seed = SEED,
  chains = 4L,
  parallel_chains = 4L,
  refresh = 0,
  thin = 1
)
```

\noindent
Per comodità, trasformiamo l'oggetto `fit` in un oggetto di classe `stanfit`:

```{r}
stanfit <- rstan::read_stan_csv(fit$output_files())
```

\noindent
L'esatta distribuzione a posteriori è una Beta(19, 15):

```{r}
summarize_beta_binomial(alpha = 2, beta = 2, y = 17, n = 30)
```

```{r}
plot_beta(alpha = 19, beta = 15) + 
  lims(x = c(0, 1))
```

\noindent
L'approssimazione della distribuzione a posteriori per $\theta$ ottenuta mediante la simulazione MCMC è

```{r}
mcmc_dens(stanfit, pars = "theta") + 
  lims(x = c(0, 1))
```

\noindent
La funzione `tidy()` nel pacchetto `broom.mixed` fornisce alcune utili statistiche per i 16000 valori della catena Markov memorizzati in `stanfit`:

```{r}
broom.mixed::tidy(
  stanfit, 
  conf.int = TRUE, 
  conf.level = 0.95, 
  pars = "theta"
)
```

\noindent
laddove, per esempio, la media esatta della corretta distribuzione a posteriori  è

```{r}
19 / (19 + 15)
```

\noindent
La funzione `tidy()` non consente di calcolare altre statistiche descrittive, oltre alla media. Ma questo risultato può essere ottenuto direttamente utilizzando i valori delle catene di Markov. Iniziamo ad esaminare il contenuto dell'oggetto `stanfit`:

```{r}
list_of_draws <- extract(stanfit)
print(names(list_of_draws))
```

\noindent
Possiamo estrarre i campioni della distribuzione a posteriori nel modo seguente:

```{r}
head(list_of_draws$theta)
```

\noindent
Creiamo un data.frame con le stime a posteriori $\hat{\theta}$:

```{r}
df <- data.frame(
  theta = list_of_draws$theta
)
```

\noindent
Le statistiche descrittive della distribuzione a posteriori possono ora essere ottenute usando direttamente i valori $\hat{\theta}$:

```{r}
df %>% 
  summarize(
    post_mean = mean(theta), 
    post_median = median(theta),
    post_mode = sample_mode(theta),
    lower_95 = quantile(theta, 0.025),
    upper_95 = quantile(theta, 0.975)
  )
```

\noindent
È anche possibile calcolare, ad esempio, la probabilità di $\hat{\theta} > 0.5$:

```{r}
df %>% 
  mutate(exceeds = theta > 0.50) %>% 
  janitor::tabyl(exceeds)
```

Occupiamoci ora della distribuzione predittiva a posteriori.

\noindent
Usando l'oggetto `stanfit` creiamo `y_bern`:

```{r}
y_bern <- list_of_draws$y_rep
dim(y_bern)
head(y_bern)
```

Dato che il codice Stan definisce un modello per i dati grezzi (ovvero, per ciascuna singola prova Bernoulliana del campione), ogni riga di `y_bern` include 30 colonne, ciascuna delle quali è una stima di un nuovo futuro valore possibile $y_i, \in \{0, 1\}$. Per ottenere `y_rep`, ovvero, il numero previsto di "successi" in nuove future $N = 30$ prove è sufficiente calcolare la somma dei valori di ciascuna riga. Ripetendo questa operazione per tutte le 16000 righe otteniamo una stima della distribuzione predittiva a posteriori:

```{r}
data.frame(y_rep = rowSums(y_bern)) %>% 
  ggplot(aes(x = y_rep)) + 
  stat_count()
```


### Posterior predictive checks

La distribuzione predittiva a posteriori viene utilizzata per eseguire i cosiddetti _Posterior Predictive Checks_ (PPC). I PPC vengono utilizzati per valutare l'_accuratezza predittiva_ del modello, ovvero per confrontare con metodi grafici la distribuzione dei dati osservati $y$ con la stima della distribuzione predittiva a posteriori $p(y^{rep} \mid y)$. Confrontando visivamente gli aspetti chiave dei dati previsti futuri $y^{rep}$ e dei dati osservati $y$ possiamo determinare rapidamente se il modello è adeguato. Se il modello si adatta bene ai dati, allora la distribuzione di $y^{rep}$ è molto simile alla distribuzione dei dati osservati. Con altre parole, i dati osservati devono sembrare plausibili alla luce della distribuzione predittiva a posteriori.

Oltre al confronto tra le distribuzioni di $y$ e $y^{rep}$ è anche possibile un confronto tra la distribuzione di varie statistiche descrittive, i cui valori sono calcolati su diversi campioni $y^{rep}$, e le corrispondenti statistiche descrittive calcolate sui dati osservati. Vengono solitamente considerate statistiche descrittive quali la media, la varianza, la deviazione standard, il minimo o il massimo. Ma confronti di questo tipo sono possibili per qualunque statistica descrittiva. Questi confronti sono appunto chiamati PPC. 

Per l'esempio presente, una volta eseguito il campionamento MCMC e ottenuto un oggetto di classe `stanfit`, è possibile usare le funzionalità del pacchetto `bayesplot` per eseguire i PPC. Nel caso presente, il campione di dati ha dimensioni esigue, per cui i PPC rifletteranno la grande incertenzza dell'inferenza.

\noindent
Dall'oggetto `stanfit` così trovato estraiamo $y^{rep}$:

```{r}
y_rep <- as.matrix(stanfit, pars = "y_rep")
dim(y_rep)
```

\noindent
Qui sotto esaminiamo la distribuzione $y$ insieme alla distribuzione di 8 campioni $y^{rep}$:

```{r}
ppc_hist(data_list$y, y_rep[1:8, ], binwidth = 1)
```

\noindent
Vediamo che la corrispondenza tra le due distribuzioni, $y$ e $y^{rep}$, è solo parziale.  

\noindent
Il confronto è più facile se sovrapponiamo graficamente la funzione di densità empirica di $y$ e quella di un insieme di campioni $y^{rep}$ (qui 50):

```{r}
ppc_dens_overlay(data_list$y, y_rep[1:50, ])
```

\noindent
Anche qui vediamo che c'è una corrispondenza solo approssimativa tra la funzione di densità empirica di $y$ e quella di un insieme di campioni $y^{rep}$ -- ciò è dovuto al fatto che il campione è molto piccolo.

\noindent
La distribuzione predittiva a posteriori è comunque in grado di rappresentare accuratamente la media di $y$:

```{r}
ppc_stat(data_list$y, y_rep, stat = "mean")
```

\noindent
Lo stesso si può dire della varianza:

```{r}
ppc_stat(data_list$y, y_rep, stat = "var")
```

\noindent
Nell'esempio successivo vedremo come i PPC possano fornirci delle indicazioni sulla mancanza di adattamento del modello ai dati quando viene considerato un campione più grande.


### PPC per il modello di Poisson

Le istruzioni R fornite qui sotto sono state recuperate dalla seguente [pagina web](http://avehtari.github.io/BDA_R_demos/demos_rstan/ppc/poisson-ppc.html#). Nell'esempio discusso da Jonah Gabry e Aki Vehtari vengono usati i seguenti dati:

```{r}
y <- c(0L, 3L, 5L, 0L, 4L, 7L, 4L, 2L, 3L, 
       6L, 7L, 0L, 0L, 3L, 7L, 5L, 5L, 0L, 
       4L, 0L, 4L, 4L, 6L, 3L, 7L, 5L, 3L, 
       0L, 0L, 2L, 0L, 1L, 0L, 1L, 5L, 4L, 
       4L, 2L, 3L, 6L, 4L, 5L, 0L, 7L, 7L, 
       4L, 4L, 4L, 0L, 6L, 1L, 5L, 6L, 5L, 
       6L, 7L, 3L, 6L, 2L, 3L, 0L, 2L, 0L, 
       6L, 6L, 0L, 3L, 4L, 4L, 5L, 5L, 0L, 
       5L, 7L, 5L, 5L, 6L, 4L, 2L, 3L, 4L, 
       6L, 4L, 6L, 6L, 4L, 0L, 6L, 5L, 5L, 
       7L, 0L, 1L, 6L, 7L, 0L, 5L, 0L, 0L, 
       5L, 6L, 5L, 1L, 0L, 7L, 1L, 2L, 6L, 
       5L, 4L, 0L, 4L, 0L, 4L, 4L, 6L, 3L, 
       0L, 0L, 3L, 3L, 4L, 2L, 5L, 3L, 4L, 
       3L, 2L, 5L, 2L, 4L, 4L, 0L, 2L, 7L, 
       5L, 7L, 5L, 5L, 7L, 7L, 0L, 4L, 6L, 
       0L, 4L, 6L, 7L, 4L, 0L, 4L, 1L, 5L, 
       0L, 3L, 5L, 7L, 6L, 0L, 5L, 5L, 6L, 
       7L, 6L, 7L, 3L, 4L, 3L, 7L, 7L, 2L, 
       5L, 4L, 5L, 5L, 0L, 6L, 2L, 4L, 5L, 
       4L, 0L, 0L, 5L, 5L, 7L, 7L, 0L, 3L, 
       0L, 3L, 3L, 6L, 1L, 4L, 2L, 0L, 4L, 
       7L, 5L, 5L, 0L, 3L, 7L, 0L, 6L, 6L, 
       4L, 1L, 6L, 7L, 6L, 0L, 3L, 6L, 4L, 
       7L, 0L, 5L, 5L, 4L, 0L, 0L, 2L, 4L, 
       6L, 0L, 5L, 0L, 2L, 7L, 2L, 7L, 5L, 
       4L, 6L, 2L, 4L, 0L, 4L, 0L, 0L, 3L, 
       5L, 4L, 3L, 5L, 5L, 7L, 7L, 0L, 6L, 
       4L, 5L, 1L, 5L, 3L, 5L, 5L, 5L, 0L, 
       2L, 7L, 6L, 2L, 3L, 2L, 5L, 4L, 7L, 
       6L, 7L, 3L, 3L, 4L, 4L, 6L, 4L, 6L, 
       7L, 1L, 5L, 6L, 3L, 3L, 6L, 3L, 4L, 
       0L, 7L, 0L, 3L, 6L, 5L, 0L, 0L, 0L, 
       5L, 4L, 4L, 0L, 4L, 7L, 5L, 5L, 3L, 
       3L, 0L, 0L, 5L, 4L, 0L, 7L, 6L, 0L, 
       6L, 2L, 0L, 6L, 1L, 0L, 4L, 0L, 4L, 
       3L, 0L, 4L, 5L, 5L, 7L, 6L, 6L, 5L, 
       4L, 7L, 0L, 6L, 4L, 7L, 7L, 5L, 0L, 
       1L, 4L, 7L, 6L, 4L, 5L, 4L, 7L, 2L, 
       5L, 2L, 6L, 3L, 2L, 7L, 4L, 3L, 4L, 
       6L, 6L, 6L, 6L, 7L, 1L, 0L, 0L, 7L, 
       7L, 4L, 2L, 4L, 5L, 5L, 7L, 4L, 1L, 
       7L, 6L, 5L, 6L, 5L, 4L, 0L, 0L, 7L, 
       0L, 0L, 5L, 6L, 6L, 3L, 6L, 0L, 0L, 
       0L, 4L, 4L, 3L, 0L, 7L, 5L, 4L, 2L, 
       7L, 0L, 4L, 0L, 0L, 2L, 4L, 5L, 0L, 
       4L, 2L, 5L, 2L, 0L, 6L, 6L, 3L, 6L, 
       0L, 2L, 5L, 0L, 0L, 0L, 6L, 0L, 0L, 
       6L, 5L, 4L, 6L, 4L, 5L, 5L, 4L, 0L, 
       3L, 4L, 3L, 3L, 5L, 3L, 4L, 5L, 7L, 
       0L, 0L, 1L, 4L, 6L, 3L, 5L, 7L, 6L, 
       6L, 5L, 0L, 5L, 4L, 0L, 0L, 2L, 6L, 
       0L, 6L, 0L, 4L, 5L, 6L, 3L, 4L, 2L, 
       3L, 4L, 0L, 5L, 0L, 0L, 0L, 0L, 3L, 
       4L, 7L, 6L, 7L, 7L, 3L, 4L, 4L, 7L, 
       4L, 5L, 2L, 5L, 6L)

N <- length(y)
```

\noindent
Per questi dati sembra appropriato un modello di Poisson.

```{r}
qplot(y)
```

\noindent
Un istogramma di un campione casuale della stessa ampiezza di $y$ tratto dalla distribuzione di Poisson è il seguente:

```{r}
x <- rpois(N, mean(y))
qplot(x)
```

\noindent
È chiaro che i due istogrammi sono molto diversi.

```{r}
plotdata <- data.frame(
  value = c(y, x), 
  variable = rep(c("Dati osservati", "Dati dalla distribuzione\n di Poisson"), each = N)
)
ggplot(plotdata, aes(x = value, color = variable)) + 
  geom_freqpoly(binwidth = 0.5) +
  scale_x_continuous(name = "", breaks = 0:max(x,y)) +
  scale_color_manual(name = "", values = c("gray30", "purple"))

```

\noindent
Anche se già sospettiamo che non sarà un buon modello per questi dati, è comunque una buona idea iniziare adattando ai dati il modello più semplice, ovvero quello di Poisson. Partendo da lì possiamo poi cercare di capire in che modo il modello è inadeguato.

```{r}
modelString <- "
data {
  int<lower=1> N;
  int<lower=0> y[N];
}
parameters {
  real<lower=0> lambda;
}
model {
  lambda ~ exponential(0.2);
  y ~ poisson(lambda);
}
generated quantities {
  int y_rep[N];
  for (n in 1:N) {
    y_rep[n] = poisson_rng(lambda);
  }
}
"
writeLines(modelString, con = "code/code_poisson.stan")
```

\noindent
Creiamo un oggetto di tipo `list` dove inserire i dati:

```{r}
data_list <- list(
  y = y,
  N = length(y)
)
```

\noindent
Adattiamo il modello ai dati:

```{r}
file <- file.path("code", "code_poisson.stan")
mod <- cmdstan_model(file)
fit <- mod$sample(
  data = data_list,
  iter_sampling = 4000L,
  iter_warmup = 2000L,
  seed = SEED,
  chains = 4L,
  parallel_chains = 4L,
  refresh = 0,
  thin = 1
)
```

\noindent
In questo modo otteniamo la seguente stima del parametro $\lambda$:

```{r}
fit$summary(c("lambda"))
```

\noindent
Confrontiamo $\lambda$ con la media di $y$:

```{r}
mean(y)
```

\noindent
Il modello trova la media giusta, ma, come vedremo nel seguito, il modello non è comunque adeguato a prevedere le altre proprietà di $y$.

\noindent
Trasformiamo l'oggetto `fit` in un oggetto `stanfit`:

```{r}
stanfit <- rstan::read_stan_csv(fit$output_files())
```

\noindent
La distribuzione a posteriori di $\lambda$ è

```{r}
lambda_draws <- as.matrix(stanfit, pars = "lambda")
mcmc_areas(lambda_draws, prob = 0.95) # color 95% interval
```

\noindent
Estraiamo $y^{rep}$ dall'oggetto `stanfit`:

```{r}
y_rep <- as.matrix(stanfit, pars = "y_rep")
dim(y_rep) 
```


#### Confronto tra l'istogramma di $y$ e gli istogrammi di diversi campioni $y^{rep}$

```{r}
ppc_hist(y, y_rep[1:8, ], binwidth = 1)
```


#### Confronto tra la funzione di densità empirica di $y$ e quella di diversi campioni $y^{rep}$

```{r}
ppc_dens_overlay(y, y_rep[1:50, ])
```


#### PPC per la media e la deviazine standard

```{r}
ppc_stat_2d(y, y_rep, stat = c("mean", "sd"))
```

Mentre la media viene riprodotta accuratamente dal modello (come avevamo visto sopra), ciò non è vero per la deviazione stanard dei dati. La domanda è quale sia l'origine di questa mancanza di adattamento.


#### Confronto tra la proporzione di zeri in $y$ e nei campioni $y^{rep}$ 


```{r}
prop_zero <- function(x) mean(x == 0)
print(prop_zero(y))
```
```{r}
ppc_stat(y, y_rep, stat = "prop_zero")
```

Da questo PPC risulta chiaro che il modello non è assolutamente in grado di catturare la proporzione di casi nei quali la variabile $Y$ assume il valore 0. In altri termini, i dati presentano un'inflazione di valori 0 rispetto a quelli che sono previsti da un modello di Poisson. Questo è un problema che si verifica spesso nei dati empirici.


#### Poisson "hurdle" model

Per ovviare il problema corrispondente all'inflazione di valori pari a 0, è possibile definire un modello di tipo "hurdle" che consente la presenza di una proporzione di valori pari a 0 maggiore di quanto normalmente previsto dalla distribuzione di Poisson. Senza entrare nei dettagli di come questo viene fatto, Gabry e  Vehtari definiscono il seguente modello:

```{r}
modelString2 <- "
data {
  int<lower=1> N;
  int<lower=0> y[N];
}
transformed data {
  int U = max(y);  // upper truncation point
}
parameters {
  real<lower=0,upper=1> theta; // Pr(y = 0)
  real<lower=0> lambda; // Poisson rate parameter (if y > 0)
}
model {
  lambda ~ exponential(0.2);
  
  for (n in 1:N) {
    if (y[n] == 0) {
      target += log(theta);  // log(Pr(y = 0))
    } else {
      target += log1m(theta);  // log(Pr(y > 0))
      y[n] ~ poisson(lambda) T[1,U];  // truncated poisson
    }
  }
}
generated quantities {
  real log_lik[N];
  int y_rep[N];
  for (n in 1:N) {
    if (bernoulli_rng(theta)) {
      y_rep[n] = 0;
    } else {
      int w;  // temporary variable
      w = poisson_rng(lambda); 
      while (w == 0 || w > U)
        w = poisson_rng(lambda);
        
      y_rep[n] = w;
    }
    if (y[n] == 0) {
      log_lik[n] = log(theta);
    } else {
      log_lik[n] = log1m(theta)
    + poisson_lpmf(y[n] | lambda)
    - log_diff_exp(poisson_lcdf(U | lambda),
                       poisson_lcdf(0 | lambda));
    }
  }
}
"
writeLines(modelString2, con = "code/code_poisson_hurdle.stan")
```

\noindent
Adattiamo il modello ai dati:

```{r}
file2 <- file.path("code", "code_poisson_hurdle.stan")
mod2 <- cmdstan_model(file2)

fit2 <- mod2$sample(
  data = data_list,
  iter_sampling = 4000L,
  iter_warmup = 2000L,
  seed = SEED,
  chains = 4L,
  parallel_chains = 4L,
  refresh = 0,
  thin = 1
)
```

\noindent
In questo caso otteniamo una stima di $\lambda$ diversa da quella ottenuta in precedenza:

```{r}
fit2$summary(c("lambda", "theta"))
```

\noindent
Il parametro $\theta$ viene usato per modellizzare l'eccesso di valori 0.


#### Confronto tra le distribuzioni a posteriori di $\lambda$ per i due modelli

```{r}
stanfit2 <- rstan::read_stan_csv(fit2$output_files())
```

```{r}
lambda_draws2 <- as.matrix(stanfit2, pars = "lambda")

lambdas <- cbind(lambda_fit1 = lambda_draws[, 1],
                 lambda_fit2 = lambda_draws2[, 1])
mcmc_areas(lambdas, prob = 0.95) # color 95% interval
```


#### Posterior predictive checks

Rifacciamo i grafici esaminati in precedenza (e alcuni altri), ma questa volta eatraendo $y^{rep}$ da `fit2`:

```{r}
y_rep2 <- as.matrix(stanfit2, pars = "y_rep")
ppc_hist(y, y_rep2[1:8, ], binwidth = 1)
```

\noindent
In questo caso la distribuzione di $y^{rep}$ è molto simile alla distribuzione di $y$.

```{r}
ppc_dens_overlay(y, y_rep2[1:50, ])
```

```{r}
ppc_stat(y, y_rep2, stat = "prop_zero")
```

```{r}
ppc_stat_2d(y, y_rep2, stat = c("mean", "sd"))
```

In conclusione, l'accuratezza predittiva del modello "hurdle" è chiaramente migliore di quella del modello di Poisson.


## Considerazioni conclusive {-}

Questo capitolo introduce le procedure di base per la manipolazione
della distribuzione a posteriori. Lo strumento fondamentale che è stato
utilizzato è quello fornito da campioni di valori dei parametri tratti
dalla distribuzione a posteriori. Lavorare con campioni di parametri
tratti dalla distribuzione a posteriori trasforma un problema di calcolo
integrale in un problema di riepilogo dei dati. Abbiamo visto quali sono
le procedure che, mediante `R`, consentono di utilizzare i campioni a
posteriori per produrre gli indici di sintesi della distribuzione a
posteriori più usati: gli intervalli di credibilità e le stime puntuali.

Abbiamo anche discusso i controlli predittivi a posteriori. A questo proposito è necessario notare un punto importante: i controlli predittivi a posteriori, quando suggeriscono un buon adattamento del modello alle caratterische dei dati previsti futuri $y^{rep}$, non forniscono una forte evidenza della capacità del modello di generalizzarsi a nuovi campioni di dati.  Infatti, una tale evidenza sulla generalizzabilità del modello può essere solo fornita da studi di _cross-validation_, ovvero da studi nei quali viene utilizzato un _nuovo_ campione di dati. D'altra parte, invece, se i PPC mostrano un cattivo  adattamento del modello ai dati previsti futuri, questo fornisce una forte evidenza di una errata specificazione del modello.
