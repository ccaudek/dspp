<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Data Science per psicologi</title>
  <meta name="description" content="Data Science per psicologi si propone di fornire un’introduzione all’analisi dei dati psicologici agli studenti del primo anno del Corso di Laurea in Scienze e Tecniche Psicologiche presso l’Università degli Studi di Firenze. Particolare attenzione sarà posta ai seguenti aspetti: l’uso del linguaggio R per lo svolgimento delle analisi statistiche, la rappresentazione grafica dei dati e l’inferenza Bayesiana." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Data Science per psicologi" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Data Science per psicologi si propone di fornire un’introduzione all’analisi dei dati psicologici agli studenti del primo anno del Corso di Laurea in Scienze e Tecniche Psicologiche presso l’Università degli Studi di Firenze. Particolare attenzione sarà posta ai seguenti aspetti: l’uso del linguaggio R per lo svolgimento delle analisi statistiche, la rappresentazione grafica dei dati e l’inferenza Bayesiana." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Data Science per psicologi" />
  
  <meta name="twitter:description" content="Data Science per psicologi si propone di fornire un’introduzione all’analisi dei dati psicologici agli studenti del primo anno del Corso di Laurea in Scienze e Tecniche Psicologiche presso l’Università degli Studi di Firenze. Particolare attenzione sarà posta ai seguenti aspetti: l’uso del linguaggio R per lo svolgimento delle analisi statistiche, la rappresentazione grafica dei dati e l’inferenza Bayesiana." />
  

<meta name="author" content="Corrado Caudek" />


<meta name="date" content="2021-09-13" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="regressione-lineare-con-stan.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Science per psicologi</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="regr-models-intro.html"><a href="regr-models-intro.html"><i class="fa fa-check"></i><b>1</b> Introduzione alla regressione lineare bayesiana</a>
<ul>
<li class="chapter" data-level="1.1" data-path="regr-models-intro.html"><a href="regr-models-intro.html#la-funzione-lineare"><i class="fa fa-check"></i><b>1.1</b> La funzione lineare</a></li>
<li class="chapter" data-level="1.2" data-path="regr-models-intro.html"><a href="regr-models-intro.html#lerrore-di-misurazione"><i class="fa fa-check"></i><b>1.2</b> L’errore di misurazione</a></li>
<li class="chapter" data-level="1.3" data-path="regr-models-intro.html"><a href="regr-models-intro.html#il-modello-di-regressione-da-una-prospettiva-bayesiana"><i class="fa fa-check"></i><b>1.3</b> Il modello di regressione da una prospettiva bayesiana</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="regr-models-intro.html"><a href="regr-models-intro.html#una-media-specifica-per-ciascuna-osservazione"><i class="fa fa-check"></i><b>1.3.1</b> Una media specifica per ciascuna osservazione</a></li>
<li class="chapter" data-level="1.3.2" data-path="regr-models-intro.html"><a href="regr-models-intro.html#relazione-lineare-tra-la-media-e-il-predittore"><i class="fa fa-check"></i><b>1.3.2</b> Relazione lineare tra la media e il predittore</a></li>
<li class="chapter" data-level="1.3.3" data-path="regr-models-intro.html"><a href="regr-models-intro.html#il-modello-di-regressione-lineare"><i class="fa fa-check"></i><b>1.3.3</b> Il modello di regressione lineare</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regr-models-intro.html"><a href="regr-models-intro.html#considerazioni-conclusive"><i class="fa fa-check"></i>Considerazioni conclusive</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regressione-lineare-con-stan.html"><a href="regressione-lineare-con-stan.html"><i class="fa fa-check"></i><b>2</b> Regressione lineare con Stan</a>
<ul>
<li class="chapter" data-level="2.1" data-path="regressione-lineare-con-stan.html"><a href="regressione-lineare-con-stan.html#interpretazione-dei-parametri"><i class="fa fa-check"></i><b>2.1</b> Interpretazione dei parametri</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="regressione-lineare-con-stan.html"><a href="regressione-lineare-con-stan.html#centrare-i-predittori"><i class="fa fa-check"></i><b>2.1.1</b> Centrare i predittori</a></li>
<li class="chapter" data-level="2.1.2" data-path="regressione-lineare-con-stan.html"><a href="regressione-lineare-con-stan.html#rappresentazione-grafica-dellincertezza-della-stima"><i class="fa fa-check"></i><b>2.1.2</b> Rappresentazione grafica dell’incertezza della stima</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="regressione-lineare-con-stan.html"><a href="regressione-lineare-con-stan.html#minimi-quadrati"><i class="fa fa-check"></i><b>2.2</b> Minimi quadrati</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="regressione-lineare-con-stan.html"><a href="regressione-lineare-con-stan.html#massima-verosimiglianza"><i class="fa fa-check"></i><b>2.2.1</b> Massima verosimiglianza</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="inferenza-sul-modello-di-regressione.html"><a href="inferenza-sul-modello-di-regressione.html"><i class="fa fa-check"></i><b>3</b> Inferenza sul modello di regressione</a>
<ul>
<li class="chapter" data-level="3.1" data-path="inferenza-sul-modello-di-regressione.html"><a href="inferenza-sul-modello-di-regressione.html#rappresentazione-grafica-dellincertezza-della-stima-1"><i class="fa fa-check"></i><b>3.1</b> Rappresentazione grafica dell’incertezza della stima</a></li>
<li class="chapter" data-level="3.2" data-path="inferenza-sul-modello-di-regressione.html"><a href="inferenza-sul-modello-di-regressione.html#intervalli-di-credibilità"><i class="fa fa-check"></i><b>3.2</b> Intervalli di credibilità</a></li>
<li class="chapter" data-level="3.3" data-path="inferenza-sul-modello-di-regressione.html"><a href="inferenza-sul-modello-di-regressione.html#rappresentazione-grafica-della-distribuzione-a-posteriori"><i class="fa fa-check"></i><b>3.3</b> Rappresentazione grafica della distribuzione a posteriori</a></li>
<li class="chapter" data-level="3.4" data-path="inferenza-sul-modello-di-regressione.html"><a href="inferenza-sul-modello-di-regressione.html#test-di-ipotesi"><i class="fa fa-check"></i><b>3.4</b> Test di ipotesi</a></li>
<li class="chapter" data-level="3.5" data-path="inferenza-sul-modello-di-regressione.html"><a href="inferenza-sul-modello-di-regressione.html#regressione-robusta"><i class="fa fa-check"></i><b>3.5</b> Regressione robusta</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="confronto-tra-due-gruppi-indipendenti.html"><a href="confronto-tra-due-gruppi-indipendenti.html"><i class="fa fa-check"></i><b>4</b> Confronto tra due gruppi indipendenti</a>
<ul>
<li class="chapter" data-level="4.1" data-path="confronto-tra-due-gruppi-indipendenti.html"><a href="confronto-tra-due-gruppi-indipendenti.html#regressione-lineare-con-una-variabile-dicotomica"><i class="fa fa-check"></i><b>4.1</b> Regressione lineare con una variabile dicotomica</a></li>
<li class="chapter" data-level="4.2" data-path="confronto-tra-due-gruppi-indipendenti.html"><a href="confronto-tra-due-gruppi-indipendenti.html#la-dimensione-delleffetto"><i class="fa fa-check"></i><b>4.2</b> La dimensione dell’effetto</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Science per psicologi</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Data Science per psicologi</h1>
<p class="author"><em>Corrado Caudek</em></p>
<p class="date"><em>2021-09-13</em></p>
</div>
<div id="regr-models-intro" class="section level1" number="1">
<h1><span class="header-section-number">Capitolo 1</span> Introduzione alla regressione lineare bayesiana</h1>
<p>Lo scopo della ricerca è trovare le associazioni tra le variabili e fare
confronti fra le condizioni sperimentali. Nel caso della psicologia, il
ricercatore vuole scoprire le leggi generali che descrivono le relazioni
tra i costrutti psicologici e le relazioni che intercorrono tra i
fenomeni psicologici e quelli non psicologici (sociali, economici,
storici, …). Abbiamo già visto come la correlazione di Pearson sia uno
strumento adatto a questo scopo. Infatti, essa ci informa sulla
direzione e sull’intensità della relazione lineare tra due variabili.
Tuttavia, la correlazione non è sufficiente, in quanto il ricercatore ha
a disposizione solo i dati di un campione, mentre vorrebbe descrivere la
relazione tra le variabili nella popolazione. A causa della variabilità
campionaria, le proprietà dei campioni sono necessariamente diverse da
quelle della popolazione: ciò che si può osservare nella popolazione
potrebbe non emergere nel campione e, al contrario, il campione
manifesta caratteristiche che non sono necessariamente presenti nella
popolazione. È dunque necessario chiarire, dal punto di vista
statistico, il legame che intercorre tra le proprietà del campione e le
proprietà della popolazione da cui esso è stato estratto.
Il modello di regressione utilizza la funzione matematica più semplice
per descrivere la relazione fra due variabili, ovvero la funzione
lineare. In questo Capitolo vedremo come si possa fare inferenza sulla relazione tra due variabili mediante il modello di regressione bayesiano. Inizieremo a descrivere le proprietà geometriche della funzione lineare per poi utilizzare questa semplice funzione per costruire un modello statistico secondo un approccio bayesiano.</p>
<div id="la-funzione-lineare" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> La funzione lineare</h2>
<p>Iniziamo con un ripasso sulla funzione di lineare. Si chiama <em>funzione lineare</em> una funzione del tipo</p>
<p><span class="math display">\[\begin{equation}
f(x) = a + b x,
\end{equation}\]</span></p>
<p>dove <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> sono delle costanti. Il grafico di tale funzione è una retta di cui il parametro <span class="math inline">\(b\)</span> è detto <em>coefficiente angolare</em> e il parametro <span class="math inline">\(a\)</span> è detto <em>intercetta</em> con l’asse delle <span class="math inline">\(y\)</span> [infatti, la retta interseca l’asse <span class="math inline">\(y\)</span> nel punto <span class="math inline">\((0,a)\)</span>, se <span class="math inline">\(b \neq 0\)</span>].</p>
<p>Per assegnare un’interpretazione geometrica alle costanti <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> si consideri la funzione</p>
<p><span class="math display">\[\begin{equation}
y = b x.
\end{equation}\]</span></p>
<p>Tale funzione rappresenta un caso particolare, ovvero quello della <em>proporzionalità diretta</em> tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>. Il caso generale della linearità</p>
<p><span class="math display">\[\begin{equation}
y = a + b x
\end{equation}\]</span></p>
<p>non fa altro che sommare una costante <span class="math inline">\(a\)</span> a ciascuno dei valori <span class="math inline">\(y = b x\)</span>. Nella funzione lineare <span class="math inline">\(y = a + b x\)</span>, se <span class="math inline">\(b\)</span> è positivo allora <span class="math inline">\(y\)</span> aumenta al crescere di <span class="math inline">\(x\)</span>; se <span class="math inline">\(b\)</span> è negativo allora <span class="math inline">\(y\)</span> diminuisce al crescere di <span class="math inline">\(x\)</span>; se <span class="math inline">\(b=0\)</span> la retta è orizzontale, ovvero <span class="math inline">\(y\)</span> non muta al variare di <span class="math inline">\(x\)</span>.</p>
<p>Consideriamo ora il coefficiente <span class="math inline">\(b\)</span>. Si consideri un punto <span class="math inline">\(x_0\)</span> e un incremento arbitrario <span class="math inline">\(\varepsilon\)</span> come indicato nella figura <a href="regr-models-intro.html#fig:linearfunction">1.1</a>. Le differenze <span class="math inline">\(\Delta x = (x_0 + \varepsilon) - x_0\)</span> e <span class="math inline">\(\Delta y = f(x_0 + \varepsilon) - f(x_0)\)</span> sono detti <em>incrementi</em> di <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>. Il coefficiente angolare <span class="math inline">\(b\)</span> è uguale al rapporto</p>
<p><span class="math display">\[\begin{equation}
    b = \frac{\Delta y}{\Delta x} = \frac{f(x_0 + \varepsilon) - f(x_0)}{(x_0 + \varepsilon) - x_0},
\end{equation}\]</span></p>
<p>indipendentemente dalla grandezza degli incrementi <span class="math inline">\(\Delta x\)</span> e <span class="math inline">\(\Delta y\)</span>. Il modo più semplice per assegnare un’interpretazione geometrica al coefficiente angolare (o pendenza) della retta è dunque quello di porre <span class="math inline">\(\Delta x = 1\)</span>. In tali circostanze infatti <span class="math inline">\(b = \Delta y\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:linearfunction"></span>
<img src="images/linear_function.png" alt="La funzione lineare $y = a + bx$." width="70%" />
<p class="caption">
Figura 1.1: La funzione lineare <span class="math inline">\(y = a + bx\)</span>.
</p>
</div>
</div>
<div id="lerrore-di-misurazione" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> L’errore di misurazione</h2>
<p>Per descrivere l’associazione tra due variabili, tuttavia, la funzione lineare non è sufficiente. Nel mondo empirico, infatti, la relazione tra variabili non è mai perfettamente lineare. È dunque necessario includere nel modello di regressione anche una componente d’errore, ovvero una componente della <span class="math inline">\(y\)</span> che non può essere spiegata dal modello lineare. Nel caso di due sole variabili, questo ci conduce alla seguente formulazione del modello di regressione:</p>
<p><span class="math display" id="eq:regbivpop">\[\begin{equation}
y = \alpha + \beta x + \varepsilon,
\tag{1.1}
\end{equation}\]</span></p>
<p>laddove i parametri <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span> descrivono l’associazione tra le variabili casuali <span class="math inline">\(y\)</span> e <span class="math inline">\(x\)</span>, e il termine d’errore <span class="math inline">\(\varepsilon\)</span> specifica quant’è grande la porzione della variabile <span class="math inline">\(y\)</span> che non può essere predetta nei termini di una relazione lineare con la <span class="math inline">\(x\)</span>.</p>
<p>Si noti che la <a href="regr-models-intro.html#eq:regbivpop">(1.1)</a> consente di formulare una predizione, nei termini di un modello lineare, del valore atteso della <span class="math inline">\(y\)</span> conoscendo <span class="math inline">\(x\)</span>, ovvero</p>
<p><span class="math display" id="eq:regbivpop2">\[\begin{equation}
\hat{y} = \mathbb{E}(y \mid x) = \alpha + \beta x.
\tag{1.2}
\end{equation}\]</span></p>
<p>In altri termini, se i parametri del modello (<span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span>) sono noti, allora è possibile predire la <span class="math inline">\(y\)</span> sulla base della nostra conoscenza della <span class="math inline">\(x\)</span>.
Per esempio, se conosciamo la relazione lineare tra quoziente di intelligenza ed aspettativa di vita, allora possiamo prevedere quanto a lungo vivrà una persona sulla base del suo QI. Sì, c’è una relazione lineare tra intelligenza e aspettativa di vita <span class="citation">(<a href="#ref-hambrick2015research" role="doc-biblioref">Hambrick 2015</a>)</span>! Ma quando è accurata la previsione? Ciò dipende dal termine d’errore della <a href="regr-models-intro.html#eq:regbivpop">(1.1)</a>. L’analisi di regressione fornisce un metodo per rispondere a domande di questo tipo.</p>
</div>
<div id="il-modello-di-regressione-da-una-prospettiva-bayesiana" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Il modello di regressione da una prospettiva bayesiana</h2>
<p>In precedenza abbiamo visto come sia possibile stimare i parametri di un modello bayesiano Normale nel quale le osservazioni sono indipendenti e identicamente distribuite secondo una densità Normale,</p>
<p><span class="math display" id="eq:normalsamplingmodel">\[\begin{equation}
Y_i \stackrel{i.i.d.}{\sim} \mathcal{N}(\mu, \sigma), \quad i = 1, \dots, n.
\tag{1.3}
\end{equation}\]</span></p>
<p>Il modello <a href="regr-models-intro.html#eq:normalsamplingmodel">(1.3)</a> assume che ogni <span class="math inline">\(Y_i\)</span> sia una realizzazione della stessa <span class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span>. Da un punto di vista bayesiano<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>, si assegnano distribuzioni a priori ai parametri <span class="math inline">\(\mu\)</span> e <span class="math inline">\(\sigma\)</span>, si genera la verosimiglianza in base ai dati osservati e, con queste informazioni, si generano le distribuzione a posteriori dei parametri <span class="citation">(<a href="#ref-gelman2020regression" role="doc-biblioref">Gelman, Hill, and Vehtari 2020</a>)</span>:</p>
<p><span class="math display">\[\begin{align}
Y_i \mid \mu, \sigma &amp; \stackrel{iid}{\sim} \mathcal{N}(\mu, \sigma^2)\notag\\
\mu       &amp; \sim \mathcal{N}(\mu_0, \tau^2) \notag\\
\sigma    &amp; \sim \Cauchy(x_0, \gamma) \notag
\end{align}\]</span></p>
<p>È comune però che vengano però registrate altre variabili <span class="math inline">\(x_i\)</span> che possono essere associate alla risposta di interesse <span class="math inline">\(y_i\)</span>. La variabile <span class="math inline">\(x_i\)</span> viene chiamata <em>predittore</em> (o variabile indipendente) in quanto il ricercatore è tipicamente interessato a predire il valore <span class="math inline">\(y_i\)</span> a partire da <span class="math inline">\(x_i\)</span>. Come si può estende il modello Normale della <a href="regr-models-intro.html#eq:normalsamplingmodel">(1.3)</a> per lo studio della possibile relazione tra <span class="math inline">\(y_i\)</span> e <span class="math inline">\(x_i\)</span>?</p>
<div id="una-media-specifica-per-ciascuna-osservazione" class="section level3" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> Una media specifica per ciascuna osservazione</h3>
<p>Il modello <a href="regr-models-intro.html#eq:normalsamplingmodel">(1.3)</a> assume una media <span class="math inline">\(\mu\)</span> comune per ciascuna osservazione <span class="math inline">\(Y_i\)</span>. Dal momento che desideriamo introdurre una nuova variabile <span class="math inline">\(x_i\)</span> che assume un valore specifico per ciascuna osservazione <span class="math inline">\(y_i\)</span>, il modello <a href="regr-models-intro.html#eq:normalsamplingmodel">(1.3)</a> può essere modificato in modo che la media comune <span class="math inline">\(\mu\)</span> venga sostituita da una media <span class="math inline">\(\mu_i\)</span> specifica a ciascuna <span class="math inline">\(i\)</span>-esima osservazione:</p>
<p><span class="math display" id="eq:normalsamplinglinearmodel">\[\begin{equation}
Y_i \mid \mu_i, \sigma \stackrel{ind}{\sim} \mathcal{N}(\mu_i, \sigma), \quad i = 1, \dots, n.
\tag{1.4}
\end{equation}\]</span></p>
<p>Si noti che le osservazioni <span class="math inline">\(Y_1, \dots, Y_n\)</span> non sono più identicamente distribuite poiché hanno medie diverse, ma sono ancora indipendenti come indicato dalla notazione <code>ind</code> posta sopra il simbolo <span class="math inline">\(\sim\)</span> nella <a href="regr-models-intro.html#eq:normalsamplinglinearmodel">(1.4)</a></p>
</div>
<div id="relazione-lineare-tra-la-media-e-il-predittore" class="section level3" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> Relazione lineare tra la media e il predittore</h3>
<p>L’approccio che consente di mettere in relazione un predittore <span class="math inline">\(x_i\)</span> con la risposta <span class="math inline">\(Y_i\)</span> è quello di assumere che la media di ciascuna <span class="math inline">\(Y_i\)</span>, ovvero <span class="math inline">\(\mu_i\)</span>, sia una funzione lineare del predittore <span class="math inline">\(x_i\)</span>. Una tale relazione lineare è scritta come</p>
<p><span class="math display" id="eq:regmodel">\[\begin{equation}
\mu_i = \beta_0 + \beta_ 1 x_i, \quad i = 1, \dots, n.
\tag{1.5}
\end{equation}\]</span></p>
<p>Nella <a href="regr-models-intro.html#eq:regmodel">(1.5)</a>, ciascuna <span class="math inline">\(x_i\)</span> è una costante nota (ecco perché viene usata una lettera minuscola per la <span class="math inline">\(x\)</span>) e <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_ 1\)</span> sono parametri incogniti. Questi parametri che rappresentano l’intercetta e la pendenza della retta di regressione sono variabili casuali. Si assegna una distribuzione a priori a <span class="math inline">\(\beta_0\)</span> e a <span class="math inline">\(\beta_ 1\)</span> e si esegue l’inferenza riassumendo la distribuzione a posteriori di questi parametri.</p>
<p>In questo modello, la funzione lineare <span class="math inline">\(\beta_0 + \beta_ 1 x_i\)</span> è interpretata come il valore atteso della <span class="math inline">\(Y_i\)</span> per ciascun valore <span class="math inline">\(x_i\)</span>, mentre l’intercetta <span class="math inline">\(\beta_0\)</span> rappresenta il valore atteso della <span class="math inline">\(Y_i\)</span> quando <span class="math inline">\(x_i = 0\)</span>. Il parametro <span class="math inline">\(\beta_ 1\)</span> (pendenza) rappresenta invece l’aumento medio della <span class="math inline">\(Y_i\)</span> quando <span class="math inline">\(x_i\)</span> aumenta di un’unità. È importante notare che la relazione lineare <a href="regr-models-intro.html#eq:normalsamplinglinearmodel">(1.4)</a> di parametri <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_ 1\)</span> descrive l’associazione tra <strong>la media</strong> <span class="math inline">\(\mu_i\)</span> e il predittore <span class="math inline">\(x_i\)</span>. In altri termini, tale relazione lineare ci fornisce una predizione sul valore medio <span class="math inline">\(\mu_i\)</span>, non sul valore <em>effettivo</em> <span class="math inline">\(Y_i\)</span>.</p>
</div>
<div id="il-modello-di-regressione-lineare" class="section level3" number="1.3.3">
<h3><span class="header-section-number">1.3.3</span> Il modello di regressione lineare</h3>
<p>Sostituendo la <a href="regr-models-intro.html#eq:regmodel">(1.5)</a> nel modello <a href="regr-models-intro.html#eq:normalsamplinglinearmodel">(1.4)</a> otteniamo il modello di regressione lineare:</p>
<p><span class="math display" id="eq:samplinglinearmodel">\[\begin{equation}
Y_i \mid \beta_0, \beta_ 1, \sigma \stackrel{ind}{\sim} \mathcal{N}(\beta_0 + \beta_ 1 x_i, \sigma), \quad i = 1, \dots, n.
\tag{1.6}
\end{equation}\]</span></p>
<p>Questo è un caso speciale del modello di campionamento Normale, dove le <span class="math inline">\(Y_i\)</span> seguono indipendentemente una densità Normale con una media (<span class="math inline">\(\beta_0 + \beta_ 1 x_i\)</span>) specifica per ciascuna osservazione e con una deviazione standard (<span class="math inline">\(\sigma\)</span>) comune a tutte le osservazioni. Poiché include un solo predittore (<span class="math inline">\(x\)</span>), questo modello è comunemente chiamato <em>modello di regressione lineare semplice</em>.</p>
<p>In maniera equivalente, il modello <a href="regr-models-intro.html#eq:samplinglinearmodel">(1.6)</a> può essere formulato come</p>
<p><span class="math display" id="eq:samplinglinearmodel2">\[\begin{equation}
Y_i = \mu_i + \varepsilon_i, \quad i = 1, \dots, n,
\tag{1.7}
\end{equation}\]</span></p>
<p>dove la risposta media è <span class="math inline">\(\mu_i = \beta_0 + \beta_ 1 x_i\)</span> e i residui <span class="math inline">\(\varepsilon_1, \dots, \varepsilon_n\)</span> sono i.i.d. da una Normale con media 0 e deviazione standard <span class="math inline">\(\sigma\)</span>.</p>
<p><img src="051_reglin1_files/figure-html/unnamed-chunk-2-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Nel modello di regressione lineare, l’osservazione <span class="math inline">\(Y_i\)</span> è una variabile casuale, il predittore <span class="math inline">\(x_i\)</span> è una costante fissa, e <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> e <span class="math inline">\(\sigma\)</span> sono parametri incogniti. Utilizzando il paradigma bayesiano, viene assegnata una distribuzione a priori congiunta a <span class="math inline">\((\beta_0, \beta_1, \sigma)\)</span>. Dopo avere osservato le risposte <span class="math inline">\(Y_i, i = 1, \dots, n\)</span>, l’inferenza procede stimando la distribuzione a posteriori dei parametri.</p>
<div class="rmdnote">
<p>Nella costruzione di un modello di regressione bayesiano, è importante iniziare dalle basi e procedere un passo alla volta. Sia <span class="math inline">\(Y\)</span> una variabile di risposta e
sia <span class="math inline">\(x\)</span> un predittore o un insieme di predittori. È possibile costruire un modello di regressione di <span class="math inline">\(Y\)</span> su <span class="math inline">\(x\)</span> applicando i seguenti principi generali:</p>
<ul>
<li>Stabilire se <span class="math inline">\(Y\)</span> è discreto o continuo. Di conseguenza, identificare l’appropriata struttura dei dati (per esempio, Normale, di Poisson, o Binomiale).</li>
<li>Esprimere la media di <span class="math inline">\(Y\)</span> come funzione dei predittori <span class="math inline">\(x\)</span> (per esempio, <span class="math inline">\(\mu = \beta_0 + \beta_1 x\)</span>).</li>
<li>Identificare tutti i parametri incogniti del modello (per esempio, <span class="math inline">\(\mu, \beta_1, \beta_2\)</span>).</li>
<li>Valutare quali valori che ciascuno di questi parametri potrebbe assumere. Di conseguenza, identificare le distribuzioni a priori appropriate per questi parametri.</li>
</ul>
</div>
<p>Nel caso di una variabile <span class="math inline">\(Y\)</span> continua che segue la legge Normale e un solo predittore, ad esempio, il modello diventa:</p>
<p><span class="math display">\[\begin{align} 
Y_i \mid \beta_0, \beta_1, \sigma  &amp;\stackrel{ind}{\sim} \mathcal{N}\left(\mu_i, \sigma^2\right) \;\; \text{ con } \;\; \mu_i = \beta_0 + \beta_1 x_i \notag\\
\beta_0  &amp;\sim \mathcal{N}\left(\mu_0, \sigma_0^2 \right)  \notag\\
\beta_1  &amp; \sim \mathcal{N}\left(\mu_1, \sigma_1^2 \right) \notag\\
\sigma &amp; \sim \text{Cauchy}(x_0, \gamma) \; .\notag
\end{align}\]</span></p>
<p>Un algoritmo MCMC viene usato per simulare i campioni dalle distribuzioni a posteriori e, mediante tali campioni, si fanno inferenze sulla risposta attesa <span class="math inline">\(\beta_0 + \beta_1 x\)</span> per ciascuno specifico valore del predittore <span class="math inline">\(x\)</span>. Inoltre, è possibile valutare le dimensioni degli errori di previsione mediante un indice sintetico della densità a posteriori della deviazione standard <span class="math inline">\(\sigma\)</span>.</p>
</div>
</div>
<div id="considerazioni-conclusive" class="section level2 unnumbered">
<h2>Considerazioni conclusive</h2>
<p>Il modello di regressione lineare semplice viene usato per descrivere la
relazione tra due variabili e per determinare il segno e l’intensità di
tale relazione. Inoltre, il modello di regressione ci consente di
prevedere il valore della variabile dipendente in base ad alcuni nuovi
valori della variabile indipendente. Il modello di regressione lineare
semplice è in realtà molto limitato, in quanto descrive soltanto la
relazione tra la variabile dipendente <span class="math inline">\(y\)</span> e una sola variabile
esplicativa <span class="math inline">\(x\)</span>. Esso diventa molto più utile quando incorpora più
variabili indipendenti. In questo secondo caso, però, i calcoli per la
stima dei coefficienti del modello diventano più complicati. Abbiamo
deciso di iniziare considerando il modello di regressione lineare semplice
perché, in questo caso, sia la logica dell’inferenza sia le procedure di
calcolo sono facilmente maneggiabili. Nel caso più generale, quello del
modello di regressione multipla, la logica dell’inferenza rimarrà
identica a quella discussa qui, ma le procedure di calcolo richiedono
l’uso dell’algebra matriciale. Il modello di regressione multipla può includere sia regressori quantitativi, sia regressori qualitativi, utilizzando un
opportuna schema di codifica. È interessante notare come un modello di
regressione multipla che include una sola variabile esplicativa
quantitativa corrisponde all’analisi della varianza ad una via; un
modello di regressione multipla che include più di una variabile
esplicativa quantitativa corrisponde all’analisi della varianza più vie.
Possiamo qui concludere dicendo che il modello di regressione, nelle sue varie forme e varianti, costituisce la tecnica di analisi dei dati maggiormente usata in psicologia.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-gelman2020regression" class="csl-entry">
Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. <em>Regression and Other Stories</em>. Cambridge University Press.
</div>
<div id="ref-hambrick2015research" class="csl-entry">
Hambrick, DZ. 2015. <span>“Research Confirms a Link Between Intelligence and Life Expectancy.”</span> <em>Scientific American. Retrieved from Http://Www. Scientificamerican. Com/Article/Research-Confirms-a-Link-Between-Intelligence-and-Life-Expectancy</em>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Per un’introduzione alla trattazione frequentista dell’analisi di regressione, si veda l’Appendice <a href="#least-squares"><strong>??</strong></a>.<a href="regr-models-intro.html#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="regressione-lineare-con-stan.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/051_reglin1.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Data Science per psicologi.pdf", "Data Science per psicologi.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
